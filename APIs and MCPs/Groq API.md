Yes, Groq does offer a free tier for their API, and they are known for their extremely fast inference speeds.

## Groq API Free Tier
Groq provides a free tier with the following characteristics:
- Zero cost to get started with basic API access[3][8]
- Access to various LLMs, including Llama models
- Limited by rate restrictions rather than a credit-based system[9][11][13]

## Rate Limits on Free Tier
The free tier has specific usage limits:
- Limited requests per minute (RPM) and per day (RPD)
- Limited tokens per minute (TPM) and per day (TPD)
- Some reports indicate the token limit per day can be around 131,072 tokens for certain models[11]

## Speed Performance
Groq is exceptionally fast compared to other AI API providers:
- Their LPU (Language Processing Unit) Inference Engine delivers industry-leading speeds[2][6]
- Independent benchmarks show Groq outperforming competitors in throughput metrics[6]
- Some models achieve speeds of:
  - Llama 3 8B: 1,250 tokens per second[1][14]
  - Llama 4 Scout (17Bx16E): 460 tokens per second[1][14] 
  - Llama 3.1 8B Instant: 750 tokens per second[1][14]

In one benchmark, Groq's implementation of Llama 2 Chat (70B) achieved 241 tokens per second, which was more than double the speed of other providers and required expanding the benchmark chart's axes to accommodate Groq's performance[6].

## Paid Tiers
If you need more capacity than the free tier offers:
- Developer Tier: Pay-as-you-go pricing with increased rate limits (up to 10x the free tier)[7][10][15]
- Enterprise Tier: For large-scale custom solutions[3][4]
- Batch API: Available for Developer Tier customers with a 25% discount (currently 50% off through April 2025)[1][7]

The Groq API is both free to start using and significantly faster than most competitors. If you're interested in high-speed AI inference, it's definitely worth trying out.

Citations:
[1] https://groq.com/pricing/
[2] https://groq.com
[3] https://console.groq.com/settings/billing/plans
[4] https://groq.com/groqcloud/
[5] https://rows.com/calculators/llm-api-price-calculator
[6] https://groq.com/artificialanalysis-ai-llm-benchmark-doubles-axis-to-fit-new-groq-lpu-inference-engine-performance-results/
[7] https://groq.com/developer-tier-now-available-on-groqcloud/
[8] https://www.byteplus.com/en/topic/404714
[9] https://console.groq.com/docs/rate-limits
[10] https://groq.com/developer-tier-now-available-on-groqcloud/
[11] https://www.reddit.com/r/LocalLLaMA/comments/1eabwvr/this_is_the_freetier_rate_limits_for_llama31_405b/
[12] https://console.groq.com/dashboard/limits
[13] https://www.byteplus.com/en/topic/448356
[14] https://groq.com/pricing/
[15] https://groq.com/self-serve-support/
[16] https://www.byteplus.com/en/topic/447736
[17] https://groq.com/news_press/groq-lpu-inference-engine-leads-in-first-independent-llm-benchmark/
[18] https://console.groq.com/docs/rate-limits
[19] https://www.reddit.com/r/LocalLLaMA/comments/1aviqk0/anyone_get_groq_api_access_yet_is_it_just_as_fast/
[20] https://groq.com/inference/
[21] https://roadmap.embedmyreviews.com/p/groq-ai-api-with-free-basic-credits-for-testing
[22] https://x.com/GroqInc/status/1757816708386205763?lang=en
[23] https://semianalysis.com/2024/02/21/groq-inference-tokenomics-speed-but/
[24] https://www.byteplus.com/en/topic/447715
[25] https://www.chipstrat.com/p/groqs-business-model-part-1-inference
[26] https://www.sanity.io/docs/high-performance-groq
[27] https://console.groq.com/dashboard/limits
[28] https://www.reddit.com/r/singularity/comments/1c845st/the_speed_at_which_groq_chips_run_llama3_is/
[29] https://console.groq.com/docs/api-reference
[30] https://www.postman.com/postman-student-programs/groq-cloud-api/collection/7c0wue2/groq-cloud-api

---
Answer from Perplexity: pplx.ai/share

