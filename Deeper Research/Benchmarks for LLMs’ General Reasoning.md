Great — I’ll explore alternative benchmarking strategies that are more aligned with what LLMs naturally excel at: verbal reasoning, abstraction, analogical thinking, linguistic fluency, and flexible problem solving. This will include evaluations inspired by human intelligence testing (like IQ or verbal aptitude tests), as well as emerging or proposed AI-specific benchmarks designed to assess general-purpose reasoning in a model-agnostic way.

I’ll let you know as soon as the overview is ready.

# Beyond Math Word Problems: Benchmarks for LLMs’ General Reasoning

## Introduction  
Math word-problem benchmarks like GSM8K have driven progress in LLM reasoning, but they may not fully reflect an LLM’s general-purpose intellect. Recent studies show that LLMs often rely on pattern recall rather than genuine logical inference on math tasks – for example, simply changing the numbers or templates in a GSM8K question can sharply degrade a model’s performance ([[2410.05229] GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models](https://arxiv.org/abs/2410.05229#:~:text=the%20limitations%20of%20existing%20evaluations%2C,of%20mathematical%20reasoning%20in%20these)) ([Benchmarking Reasoning Robustness in Large Language Models](https://arxiv.org/html/2503.04550#:~:text=Jiang%20et%20al,performance%2C%20highlighting%20the%20robustness%20deficiency)). This fragility calls into question whether improving GSM8K accuracy equates to improving fundamental reasoning ([Benchmarking Reasoning Robustness in Large Language Models](https://arxiv.org/html/2503.04550#:~:text=while%20they%20can%20simulate%20abstract,performance%2C%20highlighting%20the%20robustness%20deficiency)). Indeed, LLMs can “simulate” multi-step math reasoning seen in training data without truly understanding it ([Benchmarking Reasoning Robustness in Large Language Models](https://arxiv.org/html/2503.04550#:~:text=Jiang%20et%20al,performance%2C%20highlighting%20the%20robustness%20deficiency)). As a result, many current benchmarks (often overlapping with training data) might overestimate reasoning ability by rewarding memorization ([Benchmarking Reasoning Robustness in Large Language Models](https://arxiv.org/html/2503.04550#:~:text=A%20major%20factor%20contributing%20to,rapid%20advance%20in%20reasoning%20models)). 

To more reliably measure an LLM’s *inherent strengths* – its native verbal reasoning, analogical thinking, abstract generalization, linguistic fluency, and cognitive flexibility – we need a broader set of evaluation strategies. Below we explore two complementary approaches: **human-designed cognitive tests** (e.g. IQ-style verbal exams) and **AI-native benchmarks** tailored for LLMs. For each, we outline what ability it evaluates, how it’s scored, how widely (if at all) it’s used in LLM evaluation, and its pros and cons for gauging “raw” LLM intelligence (excluding any external tools or heavy prompt engineering).

---

## Human-Designed Cognitive Tests as Benchmarks

Human IQ and aptitude tests offer inspiration for evaluating LLMs on abilities like analogy, verbal reasoning, and flexible thinking. These tests were crafted to assess human cognitive skills and may serve as proxies for measuring similar capacities in language models. Key examples include classic analogies exams and verbal reasoning subtests from standardized IQ batteries:

### Miller Analogies Test (MAT)  
- **Evaluates:** Verbal analogical reasoning and broad knowledge by requiring solvers to complete analogies (A:B :: C:__). For example, *“Bach : Composing :: Monet : ___”* – answer: *“painting”* ([Miller Analogies Test - Wikipedia](https://en.wikipedia.org/wiki/Miller_Analogies_Test#:~:text=The%20test%20aimed%20to%20measure,test%20question%20might%20have%20been)) ([Miller Analogies Test - Wikipedia](https://en.wikipedia.org/wiki/Miller_Analogies_Test#:~:text=_______.,any%20of%20the%20four%20positions)). This tests the ability to recognize relationships between concepts.  
- **Scoring:** Objective multiple-choice; score is percent correct out of 120 questions (administered in 60 minutes) ([Miller Analogies Test - Wikipedia](https://en.wikipedia.org/wiki/Miller_Analogies_Test#:~:text=The%20Miller%20Analogies%20Test%20,3)) ([Miller Analogies Test - Wikipedia](https://en.wikipedia.org/wiki/Miller_Analogies_Test#:~:text=The%20test%20aimed%20to%20measure,test%20question%20might%20have%20been)).  
- **LLM Evaluation:** Not widely used yet for AI, though the format is well-suited to LLMs. (Some analogies datasets exist in NLP research, but MAT itself has not been a standard benchmark. Any high performance should be checked against potential training overlap, since many MAT analogies involve common facts.)  
- **Strengths:** Directly targets **analogical mapping**, a core reasoning skill. It’s a long-standing, **objective** test of relational reasoning with a single correct answer per item, making evaluation straightforward. High performance would indicate an LLM can connect ideas and use world knowledge analogically (e.g. knowing Bach:music :: Monet:art) ([Miller Analogies Test - Wikipedia](https://en.wikipedia.org/wiki/Miller_Analogies_Test#:~:text=slot%20may%20appear%20in%20any,of%20the%20four%20positions)).  
- **Limitations:** MAT analogies rely on broad **cultural and factual knowledge** (e.g. famous names, scientific terms) ([Miller Analogies Test - Wikipedia](https://en.wikipedia.org/wiki/Miller_Analogies_Test#:~:text=slot%20may%20appear%20in%20any,of%20the%20four%20positions)). A language model might get items right by sheer knowledge recall or seeing similar analogies in training, rather than on-the-fly reasoning. Thus, strong MAT results may reflect a rich knowledge base more than novel reasoning. Moreover, MAT was a **multiple-choice** test – there’s a risk the model could exploit the provided options or linguistic patterns rather than truly reason. (As a human analogy test, MAT was discontinued in 2023, partly due to concerns that it favored test-specific prep and knowledge.) For LLM evaluation, ensuring the analogies aren’t memorized is crucial.

### WAIS Verbal Reasoning Subtests (Similarities, Comprehension, etc.)  
*Wechsler Adult Intelligence Scale (WAIS)* is a gold-standard IQ test with several verbal sub-components that could be repurposed to probe LLMs:  

- **Similarities** – **Evaluates:** abstract verbal reasoning and concept formation. The test presents two seemingly unrelated words and asks how they are alike (e.g. *“In what way are a **ship** and **train** alike?”*). This measures the ability to identify an abstract similarity or category ([Verbal Comprehension in WAIS: Skills and Subtests | Cogn-IQ.org](https://www.cogn-iq.org/wais-verbal-comprehension.php#:~:text=2)).  
  **Scoring:** Human-rated (responses scored 0, 1, or 2 points based on quality).  
  **Use in LLM Eval:** Only anecdotal experiments. An LLM can be prompted with such questions and its answers compared to expected ones.  
  **Strengths:** Tests **abstract generalization** – the model must go beyond surface to find an underlying concept (ship and train are both transportation). This aligns with LLMs’ strength in semantic generalization. It also avoids simple lookup because open-ended phrasing means the model must generate the concept.  
  **Limitations:** Scoring is subjective – requires comparing the model’s answer to an expected answer (which might need human judgment if phrased differently). Also, many similarity pairs (especially common ones like ship/train) are likely present in training data or encyclopedic knowledge, so top-tier models might answer correctly by recall. It’s hard to ensure the test distinguishes reasoning from memorized facts. Additionally, because the model *always* produces an answer (no “I don’t know”), it might give a plausible-sounding but off-base similarity – needing careful human evaluation of correctness.

- **Comprehension** – **Evaluates:** practical reasoning and common-sense understanding of situations (a typical item might ask *“Why do we wear seat belts?”* expecting a reasoning about safety).  
  **Scoring:** Human-rated (0–2 points).  
  **LLM Use:** Limited. In principle, LLMs can answer these open-ended “why” or “what should one do” questions easily.  
  **Strengths:** Measures **commonsense reasoning and judgment** in everyday scenarios. LLMs have absorbed vast common wisdom from text and often excel at such questions (e.g. GPT-4 can articulate multiple sensible reasons for wearing seat belts). Good performance would highlight the model’s grasp of cause-effect and social norms in language.  
  **Limitations:** Again subjective scoring. Also, these questions may be too easy for current large models – they tap knowledge that is plentiful in training data (e.g. safety advice). Most state-of-the-art LLMs will answer such questions correctly almost every time, so the test may not differentiate high-tier models (a ceiling effect).

- **Vocabulary** – **Evaluates:** depth of word knowledge (definitions of words).  
  **Scoring:** Human-rated on quality of definition.  
  **LLM Use:** Not formally tested, but trivially, language models excel at defining words (often better than many humans).  
  **Strengths:** Shows linguistic **fluency and knowledge**. An LLM’s vocabulary breadth is usually enormous, so strong performance demonstrates one aspect of verbal ability.  
  **Limitations:** This mostly measures knowledge, not reasoning. It would likely be too easy: e.g., GPT-4 can define obscure words effortlessly. Vocabulary alone is not a proxy for reasoning power in LLMs (and indeed, in humans high vocabulary correlates with education more than raw problem-solving skill).

*Overall,* WAIS verbal subtests (part of the Verbal Comprehension Index) target **verbal reasoning, concept abstraction, and general knowledge** ([Verbal Comprehension in WAIS: Skills and Subtests | Cogn-IQ.org](https://www.cogn-iq.org/wais-verbal-comprehension.php#:~:text=This%20subtest%20assesses%20abstract%20thinking,to%20identify%20relationships%20between%20concepts)) ([Verbal Comprehension in WAIS: Skills and Subtests | Cogn-IQ.org](https://www.cogn-iq.org/wais-verbal-comprehension.php#:~:text=3)). They are **not widely adopted in LLM evaluation** so far, partly because their open-ended nature requires human scoring or carefully designed automated rubrics. However, their **strength** lies in assessing how well an LLM can **think with language**: finding abstract commonalities, explaining concepts, and applying common sense – skills very natural to these models. The **limitations** are that many questions draw on well-trodden knowledge or appear in training data, and scoring consistency can be an issue. Still, a tailored evaluation using these subtests (with a predefined answer key or rubric for automated checking) could reveal nuance in an LLM’s **verbal reasoning prowess** beyond what a math word problem can show.

### Jouve-Cerebrals Word Similarities Test (JCWS)  
- **Evaluates:** A broad spectrum of verbal reasoning skills through *open-ended* analogy and association tasks. JCWS is an IQ test specifically designed to measure **verbal logic, associative thinking, and pattern recognition** with language ([Assess Verbal Reasoning with the JCWS | Cogn-IQ](https://www.cogn-iq.org/word-similarities-iq-test.php#:~:text=The%20JCWS%20test%20assesses%20several,key%20verbal%20reasoning%20skills)). It has three sub-parts: (1) **“Nearly the Same”** – provide synonyms or related words for a clue; (2) **“Is To As”** – complete analogies (e.g. find a word that fits A:B :: C:__); (3) **“Which Relates To”** – identify a word that fits a sequence or logical progression ([Assess Verbal Reasoning with the JCWS | Cogn-IQ](https://www.cogn-iq.org/word-similarities-iq-test.php#:~:text=The%20JCWS%20test%20consists%20of,three%20subtests)). This setup forces the test-taker (or model) to *generate* answers (not just pick from choices), engaging flexible verbal reasoning.  
- **Scoring:** Open-response scoring with a predefined key for acceptable answers. In human use, it’s carefully validated (high internal consistency, α≈0.95) ([Assess Verbal Reasoning with the JCWS | Cogn-IQ](https://www.cogn-iq.org/word-similarities-iq-test.php#:~:text=Reliability)). An LLM’s responses could be auto-checked against the key or evaluated for semantic correctness.  
- **LLM Evaluation:** JCWS is **very new** (developed 2023) ([Assess Verbal Reasoning with the JCWS | Cogn-IQ](https://www.cogn-iq.org/word-similarities-iq-test.php#:~:text=,norms.html)) and hasn’t yet been applied to LLMs in published research. However, it closely mirrors tasks (analogies, finding relations) that LLMs are intuitively good at. One could administer JCWS questions to an LLM to see how well it handles open-ended analogies and word relationships.  
- **Strengths:** Because it’s **open-ended**, it avoids the multiple-choice guessing issue and requires the model to truly *produce* a correct relationship or analogy. It targets **analogy-solving and pattern completion** in language – arguably core “intelligence” skills. Also, JCWS was shown to correlate strongly with the WAIS Verbal IQ in humans (r > 0.8) ([Assess Verbal Reasoning with the JCWS | Cogn-IQ](https://www.cogn-iq.org/word-similarities-iq-test.php#:~:text=,alignment%20with%20similar%20verbal%20constructs)), meaning it measures similar cognitive abilities; if an LLM does well, it’s plausible it has human-like verbal reasoning strength in that domain. The test also emphasizes **associative fluency** (generating related words, which for an LLM might showcase the richness of its semantic network).  
- **Limitations:** Automatic scoring can be tricky – the model might give a correct analogy in different words than the expected answer. For instance, if the intended answer is “snake” but the model says “serpent,” a naive checker might mark it wrong unless synonyms are accounted for. Also, the content of JCWS might overlap with examples the model has seen (especially in its analogies section), so distinguishing genuine reasoning from memory is a challenge. Finally, since it’s not widely used yet, we’d need to ensure the test’s difficulty is appropriate – a top-tier model might actually solve most items easily, given its enormous training on word relationships, thus we’d learn more by focusing on the more difficult items or errors.

### Other Human-Devised Tests and Tasks  
Beyond the above, several **human intelligence or creativity tests** could be leveraged to assess LLMs’ general reasoning and flexible thinking:

- **Remote Associates Test (RAT):** A classic creativity puzzle where one must find a single word associated with three given words (e.g. *“cottage – swiss – cake”* ⇒ **“cheese”**). This tests **associative reasoning** and the ability to find hidden connections. **Scoring** is objective (each item has one correct answer). **Use in LLMs:** Underexplored; a strong language model with vast knowledge might excel (indeed, such puzzles often appear in its training data or can be solved via its knowledge of common phrases like “cottage cheese,” “Swiss cheese,” etc.). **Strengths:** Measures a form of creative problem-solving (convergent thinking) without requiring tools. **Limitations:** Many RAT items rely on common compound words or phrases, so high performance might not indicate creativity so much as familiarity with idioms. Nonetheless, testing an LLM on a battery of novel RAT items could indicate how flexibly it can connect disparate concepts. 

- **Verbal Creativity and Divergent Thinking Tests:** For example, the *Torrance Tests of Creative Thinking (TTCT)* have verbal components (like asking “How many unusual uses can you think of for a brick?” or storytelling prompts). **Evaluates:** creativity in terms of **fluency** (number of ideas), **flexibility** (variety of idea categories), and **originality** (novelty of ideas). **Scoring:** Uses human or rubric-based ratings, sometimes comparing to normative data. **LLM Use:** In a recent study, researchers gave GPT-4 TTCT-style tasks and found it scored in the *top 1% of human creativity* for originality and fluency ([
The originality of machines: AI takes the Torrance Test – DOAJ](https://doaj.org/article/8f092ccc969840d181456110ec35bc2c#:~:text=This%20exploratory%20research%20investigated%20the,additional%20research%20to%20further%20define)) – meaning it produced a large number of imaginative ideas, comparable to exceptionally creative humans. **Strengths:** These tasks tap the generative and free-associative power of LLMs, showcasing **cognitive flexibility** (the ability to shift directions and produce many possibilities). They align well with what LLMs do: generate text. **Limitations:** Scoring is subjective; moreover, an LLM might produce *seemingly* original ideas that are actually recombinations of training data examples. In the cited study, GPT-4’s high creativity score suggests current tests might even be too easy for it (ceiling effect). Still, using controlled creative tasks could help differentiate lesser models or highlight aspects of “thinking outside the box” that aren’t captured by logic puzzles.

- **Common-Sense and Logical Reasoning Exams:** Standardized test questions from exams like the **LSAT** or **GMAT** can serve as challenging benchmarks. For instance, **LSAT Logical Reasoning** problems present a short argument and ask for an analysis (identify an assumption, choose the best conclusion, etc.), and **Logic Games** require deducing an arrangement given rules. **Evaluates:** deductive and inductive reasoning in natural language. **Scoring:** Objective (multiple-choice). **Use in LLMs:** The GPT-4 model has already demonstrated above-average performance on LSAT sections in zero or few-shot settings (even approaching or surpassing the 80th percentile in some cases) ([[2210.09261] Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them](https://arxiv.org/abs/2210.09261#:~:text=,performance%20and%20capabilities%20of%20language)). **Strengths:** These are *human-level* reasoning challenges that require understanding complex language and logical relationships without external tools. Success indicates a model can parse and reason about novel scenarios. **Limitations:** Top LLMs have begun to master many of these, often via chain-of-thought prompting or fine-tuning; also, the multiple-choice format means an LLM might do well by elimination strategies or subtle biases in the answer choices. Moreover, these tests blend reasoning with some linguistic tricks, which LLMs might pick up on without truly replicating human thought processes. Still, their **broad adoption** in evaluating advanced models (e.g. GPT-4’s technical report) makes them relevant benchmarks for “general reasoning.” As models saturate these, we again see the need for even harder or more diverse tests.

**Summary of Human-Test Benchmarks:** Human-designed assessments like analogies, verbal IQ subtests, creativity tests, and logic exams can **fill gaps left by math word problems**. They align with LLMs’ natural linguistic abilities – testing whether a model can reason with words, form analogies, draw inferences, and produce creative ideas. The **objective** ones (analogies, RAT, LSAT questions) offer clear scoring, whereas open-ended ones (similarities questions, divergent thinking tasks) provide insight into flexibility at the cost of harder evaluation. A major theme is that LLMs have often *seen* similar questions before; thus, an ideal evaluation might use the **format** of these tests while devising fresh content to truly test on-the-fly reasoning. These human-inspired benchmarks, used judiciously, are promising analogs for measuring an LLM’s “intelligence” in a way that plays to its strengths in language understanding and generation, rather than its weaknesses with arithmetic. 

---

## AI-Native Benchmarks for General-Purpose Reasoning

In addition to repurposing human tests, researchers have developed **benchmark suites specifically for AI** to probe reasoning, abstraction, and flexibility in ways that go beyond narrow domains. Unlike human tests, these are often synthetic or task-based and can be tailored to expose LLMs’ limitations. Notable efforts include the **BIG-Bench** multi-task collection and various focused challenge tasks:

### BIG-Bench and BIG-Bench Hard (BBH) – *Diverse Reasoning Tasks*  
The **Beyond the Imitation Game Benchmark (BIG-Bench)** is a crowd-sourced collection of over 200 tasks designed to test a wide array of abilities in language models ([[2210.09261] Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them](https://arxiv.org/abs/2210.09261#:~:text=%3E%20Abstract%3ABIG,for%20which%20prior%20language%20model)). These tasks range from basic language skills to high-level reasoning puzzles. A subset of 23 particularly challenging tasks was labeled **BIG-Bench Hard (BBH)** ([[2210.09261] Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them](https://arxiv.org/abs/2210.09261#:~:text=,2022)), which includes things like: logical deduction puzzles, causal judgment in short stories, disambiguating pronouns with common sense, solving simple algebraic word problems, evaluating **Dyck languages** (balanced parentheses patterns), identifying formal logical fallacies, and even creative tasks like **“Ruin Names”** (where one must alter one letter in a name to create a funny new name) ([maveriq/bigbenchhard · Datasets at Hugging Face](https://huggingface.co/datasets/maveriq/bigbenchhard#:~:text=,not)) ([maveriq/bigbenchhard · Datasets at Hugging Face](https://huggingface.co/datasets/maveriq/bigbenchhard#:~:text=,input%20and%20makes%20it%20humorous)). Each task is typically small (250 examples) with a clear objective metric (accuracy, multiple-choice correctness, etc.). 

- **Evaluates:** **General-purpose reasoning and knowledge** across many domains – from logical and algorithmic reasoning (e.g. evaluating Boolean expressions or shuffling objects and tracking positions) to commonsense and linguistic understanding (e.g. picking the plausible ending to a story, resolving ambiguous references) ([maveriq/bigbenchhard · Datasets at Hugging Face](https://huggingface.co/datasets/maveriq/bigbenchhard#:~:text=,not)) ([maveriq/bigbenchhard · Datasets at Hugging Face](https://huggingface.co/datasets/maveriq/bigbenchhard#:~:text=,to%20which%20the%20pronoun%20refers)). BBH tasks specifically were ones where even large models fell short of average human performance without special techniques ([[2210.09261] Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them](https://arxiv.org/abs/2210.09261#:~:text=the%20best%20model%20in%20the,code)), so they target the harder corners of reasoning.  
- **Scoring:** Objective for each task (exact match or multiple-choice accuracy). The benchmark often reports an aggregate, but each sub-task has its own score.  
- **Use in LLM Evaluation:** **Widely adopted in recent research.** BIG-Bench was introduced in 2022 and many LLM papers report results on it or BBH to gauge general reasoning. For example, chain-of-thought prompting was shown to dramatically improve performance on BBH tasks ([[2210.09261] Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them](https://arxiv.org/abs/2210.09261#:~:text=,performance%20and%20capabilities%20of%20language)). It’s become a **de facto standard** for stress-testing models’ reasoning breadth ([[2502.19187] BIG-Bench Extra Hard](https://ar5iv.org/pdf/2502.19187#:~:text=demanding%20robust%20general%20reasoning%20capabilities,art%20models)).  
- **Strengths:** **Diversity** – by covering dozens of different types of tasks, it checks an LLM’s *cognitive flexibility*. A model can’t ace BBH by mastering a single skill; it must handle formal logic, commonsense, arithmetic, and more. Notably, BBH revealed that models often needed multi-step reasoning: with straightforward prompting, models performed poorly, but with techniques like chain-of-thought (letting the model generate intermediate steps), performance jumped, sometimes even surpassing humans on several tasks ([[2210.09261] Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them](https://arxiv.org/abs/2210.09261#:~:text=,performance%20and%20capabilities%20of%20language)). This suggests the benchmark was indeed testing aspects of reasoning that aren’t solved by superficial pattern matching. BBH tasks are also **objective and standardized**, making them easy to evaluate automatically and compare across models.  
- **Limitations:** As with any fixed benchmark, over time models have begun to saturate it. By late 2023, the best models were getting near-perfect scores on many BBH tasks ([[2502.19187] BIG-Bench Extra Hard](https://arxiv.org/abs/2502.19187#:~:text=skills%20within%20a%20unified%20framework,for%20the%20best)), meaning those tasks no longer differentiate the top end. There’s evidence some tasks had unintended shortcuts – e.g. the original *Boolean Expressions* task could be solved by a trivial strategy or was implicitly learned, so it wasn’t truly taxing reasoning ([[2502.19187] BIG-Bench Extra Hard](https://ar5iv.org/pdf/2502.19187#:~:text=Boardgame%20QA%20Based%20on%20Kazemi,hop)). Also, since BIG-Bench was public, models trained after its release may have seen some of its data, potentially inflating results (a form of test data contamination). In short, BBH was a great general test, but **state-of-the-art models are now exceeding human level on much of it**, reducing its utility ([[2502.19187] BIG-Bench Extra Hard](https://arxiv.org/abs/2502.19187#:~:text=skills%20within%20a%20unified%20framework,for%20the%20best)). This has directly led to the creation of an even harder successor, described next.

### BIG-Bench *Extra* Hard (BBEH) – *Next-Generation Reasoning Challenges*  
 ([[2502.19187] BIG-Bench Extra Hard](https://ar5iv.org/pdf/2502.19187)) *Performance of various models on the new BBEH benchmark (harmonic mean accuracy across tasks). “General-purpose” LLMs (blue bars) like GPT-4 achieve under 10% on average, while even specialized reasoning models (red bars) peak below 45% ([[2502.19187] BIG-Bench Extra Hard](https://arxiv.org/abs/2502.19187#:~:text=Extra%20Hard%20,publicly%20at%3A%20this%20https%20URL)). This gap highlights the difficulty of BBEH’s tasks, leaving ample room for progress.*  

To raise the bar, Google DeepMind introduced **BBEH (2025)** as an extension of BBH. It **replaces each of the 23 BBH tasks with a new, more difficult task targeting the same skill** ([[2502.19187] BIG-Bench Extra Hard](https://arxiv.org/abs/2502.19187#:~:text=the,achieving%20robust%20general%20reasoning%20in)) ([[2502.19187] BIG-Bench Extra Hard](https://ar5iv.org/pdf/2502.19187#:~:text=We%20create%20BIG,benchmark%20contains%20questions%20per%20task)). For example, the simple logical ordering puzzle in BBH was replaced by a **“Boardgame QA”** task that requires many hops of deduction and even learning conflict-resolution rules on the fly ([[2502.19187] BIG-Bench Extra Hard](https://ar5iv.org/pdf/2502.19187#:~:text=BBEH%20task%20Summary%20of%20changes,expressions%20that%20evaluate)). The pronoun disambiguation was replaced with a version having much longer, more complex passages (harder coreference resolution) ([[2502.19187] BIG-Bench Extra Hard](https://ar5iv.org/pdf/2502.19187#:~:text=Causal%20Understanding%20A%20subset%20of,Commonsense%20understanding%2C%20linguistics%20knowledge)). The Dyck parentheses task was turned into finding errors in a complicated sequence (harder than just completing it) ([[2502.19187] BIG-Bench Extra Hard](https://ar5iv.org/pdf/2502.19187#:~:text=Dyck%20Language%20Involves%20finding%20errors,Replaces%20the)). And a task like Hyperbaton (judging correct adjective order) was upgraded to forcing the model to induce a *novel* adjective order rule in an alien language from examples – truly testing few-shot generalization ([[2502.19187] BIG-Bench Extra Hard](https://ar5iv.org/pdf/2502.19187#:~:text=Geometric%20Shapes%20Requires%20identifying%20the,against%20strong%20prior%2C%20linguistic%20knowledge)). In short, **BBEH is designed so that heuristic shortcuts won’t work** and the model must **engage in deep reasoning or adaptation** for each task.

- **Evaluates:** Similar broad reasoning categories as BBH – logical deduction, mathematical reasoning, commonsense, linguistics – but at a more **complex level**. Many BBEH tasks explicitly test an LLM’s ability to **learn or adapt within a single session** (a proxy for cognitive flexibility), such as inferring new rules (as in the Hyperbaton variant) or handling “needle-in-haystack” problems with lots of distractions (e.g. a task giving a large buggy table and requiring a specific query) ([[2502.19187] BIG-Bench Extra Hard](https://ar5iv.org/pdf/2502.19187#:~:text=Boardgame%20QA%20Based%20on%20Kazemi,hop)) ([[2502.19187] BIG-Bench Extra Hard](https://ar5iv.org/pdf/2502.19187#:~:text=Buggy%20Tables%20Requires%20understanding%20and,judgement%2Freasoning%2C%20logical%20reasoning%2C%20counterfactual%20reasoning)). This meta-learning aspect (learning on the fly from examples or instructions) is something LLMs can do in principle via few-shot prompting, and BBEH pushes that to the limit.  
- **Scoring:** Objective, same as BBH (accuracy per task). Results are often aggregated as a harmonic mean to avoid one easy task dominating the score ([[2502.19187] BIG-Bench Extra Hard](https://arxiv.org/abs/2502.19187#:~:text=probes%20a%20similar%20reasoning%20capability,publicly%20at%3A%20this%20https%20URL)).  
- **Use in LLM Evaluation:** **Emerging.** BBEH is brand-new (released in 2025) ([[2502.19187] BIG-Bench Extra Hard](https://arxiv.org/abs/2502.19187#:~:text=)) ([[2502.19187] BIG-Bench Extra Hard](https://arxiv.org/abs/2502.19187#:~:text=,in%20LLMs%20have%20led%20to)). Initial evaluations showed even the best GPT-4-class model achieved **<10%** average accuracy, while a bespoke “reasoning-tuned” model reached ~45% ([[2502.19187] BIG-Bench Extra Hard](https://arxiv.org/abs/2502.19187#:~:text=Extra%20Hard%20,publicly%20at%3A%20this%20https%20URL)) (see figure above). This indicates current models have a long way to go, and BBEH may become a **key benchmark going forward** to measure progress in general reasoning. It’s likely to be adopted in research shortly as a tough stress test.  
- **Strengths:** **Extremely challenging and diagnostic.** Each task was crafted to thwart superficial patterns and force genuine reasoning or abstraction. The fact that GPT-4 (a top-tier model) scores near chance on many of these tasks is actually a *feature*: it means the benchmark can **differentiate** stronger future models from current ones, acting as a high ceiling. BBEH also maintains the **diversity** of BBH ([[2502.19187] BIG-Bench Extra Hard](https://ar5iv.org/pdf/2502.19187#:~:text=we%20build%20on%20the%20success,For%20more%20details)), so it still checks a wide range of skills (logical, causal, spatial, numerical, linguistic, etc.), but in a harder way. Importantly, the tasks often involve **longer contexts or multi-step inferencing**, closer to real-world reasoning challenges than quick riddles. This could better reveal an LLM’s *raw problem-solving ability* when it can’t fall back on easy training analogies.  
- **Limitations:** The **difficulty** of BBEH is a double-edged sword. With many models scoring in single digits percent, it may be hard to diagnose *why* they fail without detailed error analysis – a model could just as well be guessing on most tasks. Thus, improvements might be hard to measure incrementally (one has to watch for small gains from, say, 10% to 15% which might still be far from mastery). Another limitation is that some BBEH tasks are quite involved (e.g. reconstructing a table from a description of a bug) – running these evaluations can be time-consuming and might require careful prompting to ensure the model even attempts the right approach. Nevertheless, these are issues any challenging test poses. BBEH’s introduction explicitly addresses the community’s need for **broader reasoning evaluation beyond math/coding** ([[2502.19187] BIG-Bench Extra Hard](https://arxiv.org/abs/2502.19187#:~:text=,in%20LLMs%20have%20led%20to)), so its breadth is a strength in aligning with LLMs’ general capabilities.

### Symbolic and Algorithmic Reasoning Tasks (Synthetic Challenges)  
Researchers have devised **targeted puzzles** to test specific reasoning mechanics in LLMs. Two famous examples introduced by Wei et al. (2022) are the **Last Letter Concatenation** task and the **Coin-Flips** task ([Understanding Reasoning in Large Language Models: Overview of the paper "Towards Reasoning in Large Language Models: A Survey" | DigitalOcean](https://www.digitalocean.com/community/tutorials/understanding-reasoning-in-llms#:~:text=accordance%20with%20strict%20rules,Letter%20Concatenation%20and%20Coin%20Flip)). These are deliberately simple algorithmic problems stated in plain language:

- In **Last Letter Concatenation**, the model is given a list of words and asked to concatenate their last letters (e.g. “Given `apple, banana, cherry`, answer should be `eay`”). This requires it to iterate through each word mentally and apply a rule – essentially testing a form of **symbolic manipulation** and working memory.  
- In **Coin-Flips**, the prompt describes a sequence of coin flip outcomes or a coin-flipping procedure and asks a question about the result (e.g. “Flip a fair coin 3 times. What is the probability of exactly 2 heads?” or a narrative version of such). This tests basic probabilistic reasoning or simulation of random events step by step.

These tasks are **not hard for a human** with a clear head, but language models initially struggled with them because they involve multi-step computation that isn’t just pattern recall. They became a benchmark for prompting techniques (like chain-of-thought prompting, where the model is guided to work out the steps explicitly) ([Understanding Reasoning in Large Language Models: Overview of the paper "Towards Reasoning in Large Language Models: A Survey" | DigitalOcean](https://www.digitalocean.com/community/tutorials/understanding-reasoning-in-llms#:~:text=Chain%20of%20Thought%20and%20Its,Variants)) ([Understanding Reasoning in Large Language Models: Overview of the paper "Towards Reasoning in Large Language Models: A Survey" | DigitalOcean](https://www.digitalocean.com/community/tutorials/understanding-reasoning-in-llms#:~:text=,balls%20does%20he%20have%20now)).

- **Evaluates:** **Symbolic reasoning and sequential processing** in a pure form. They require following a procedure (like iterating over words, or applying probability rules) that isn’t naturally present in typical text. They are good proxies for an LLM’s ability to do **implicit computation** just by reasoning in text.  
- **Scoring:** Completely objective (either the final string/number is correct or not).  
- **LLM Evaluation:** These have been used in numerous research papers as small benchmark tests. For instance, they demonstrated that a large model *without* special prompting fails at these simple tasks, but with a chain-of-thought prompt like “Let’s think step by step,” success rates dramatically improve ([Understanding Reasoning in Large Language Models: Overview of the paper "Towards Reasoning in Large Language Models: A Survey" | DigitalOcean](https://www.digitalocean.com/community/tutorials/understanding-reasoning-in-llms#:~:text=Chain%20of%20Thought%20and%20Its,Variants)) ([Understanding Reasoning in Large Language Models: Overview of the paper "Towards Reasoning in Large Language Models: A Survey" | DigitalOcean](https://www.digitalocean.com/community/tutorials/understanding-reasoning-in-llms#:~:text=,balls%20does%20he%20have%20now)). They were part of evaluating reasoning robustness in many studies and are included in some evaluation suites.  
- **Strengths:** They **isolate reasoning ability** from knowledge. A model doesn’t need any outside info to solve these – just logic. So they are a good measure of the model’s inherent capability to carry out multi-step inference internally. They also have **no ambiguity in scoring** and can be generated in unlimited quantity (one can make many instances like these with varying numbers or words, to thoroughly test consistency). Performance on such tasks correlates with the model’s **chain-of-thought capacity**; improvements indicate better internal reasoning processes ([Benchmarking Reasoning Robustness in Large Language Models](https://arxiv.org/html/2503.04550#:~:text=of,%28%2030)) ([Benchmarking Reasoning Robustness in Large Language Models](https://arxiv.org/html/2503.04550#:~:text=Jiang%20et%20al,performance%2C%20highlighting%20the%20robustness%20deficiency)).  
- **Limitations:** They are highly **unnatural tasks** from the perspective of language. It’s unlikely an LLM saw many examples of “concatenate the last letters” in its training, so it may require prompting or fine-tuning to even understand what is being asked. In that sense, failing these out-of-the-box might not mean the model lacks reasoning *potential*, just that it wasn’t conditioned to do these odd tasks. Also, once models are specifically trained or prompted for them, they become trivial – so they serve more as diagnostics than enduring benchmarks. Another limitation is that solving these doesn’t necessarily translate to solving more **semantically complex** reasoning (a model could be taught a mechanical trick for last-letter tasks but still fail at commonsense reasoning). Thus, they should be viewed as **unit tests** for certain reasoning skills rather than holistic intelligence measures.

### Relational and Analogical Reasoning in Text  
Analogy and relational mapping are hallmarks of human intelligence, and researchers have sought to evaluate LLMs on these abilities in **textual form**. Beyond simple word analogies (like MAT-style), there are efforts to test deeper analogical reasoning:

- One novel approach introduced by Musker et al. (2023) creates tasks where the model must **map between two domains** – for instance, matching a sequence of letters to a sequence of words by finding an underlying rule ([LLMs as Models for Analogical Reasoning](https://arxiv.org/html/2406.13803v2#:~:text=flexible%20representational%20and%20mapping%20capabilities,and%20semantic%20content%2C%20introducing%20variations)). In their work, they gave LLMs analogies that require **re-representing information**. A simplified example: *“The relationship between `chef` and `kitchen` is like the relationship between `doctor` and ____.”* This is a straightforward analogy (answer: `hospital`). But a harder variant might encode one side in a different form, e.g. mapping *letters* to *words* such that the structure of the analogy is less familiar. The idea is to force the model to **flexibly translate concepts** (chef→kitchen vs. doctor→hospital) rather than relying on a memorized pair.  
- Another line of work has been datasets like **CLUTRR** (a synthetic dataset of short stories requiring inferring family relationships). For example, a story might implicitly state some relations (“John is Mary’s father, and Mary is Sue’s grandmother… who is John to Sue?”). The model has to **chain multiple relations** to answer (John is Sue’s great-grandfather). This tests symbolic relational reasoning within a narrative context.

- **Evaluates:** In general, these tasks examine **analogical thinking, relational logic, and the ability to apply rules in novel combinations**. They are more **symbolic** than factual. The analogical mapping tasks specifically probe if an LLM can identify structural similarities across different contexts – a key aspect of abstract reasoning ([LLMs as Models for Analogical Reasoning](https://arxiv.org/html/2406.13803v2#:~:text=Analogical%20reasoning%E2%80%94the%20capacity%20to%20identify,this%20study%2C%20we%20introduce%20novel)) ([LLMs as Models for Analogical Reasoning](https://arxiv.org/html/2406.13803v2#:~:text=flexible%20representational%20and%20mapping%20capabilities,and%20semantic%20content%2C%20introducing%20variations)). CLUTRR-type tasks evaluate whether the model can follow logical chains and handle variables (persons) and relations – essentially a test of **deductive reasoning in language**.  
- **Scoring:** Objective (either the correct relation or analogy is produced). For analogies, one can have multiple-choice or a set of acceptable answers; for relational puzzles, there is a single correct answer.  
- **LLM Evaluation:** These have been somewhat niche but telling. Musker et al. found that advanced LLMs can sometimes match human performance on certain analogical reasoning tasks, but they also noticed differences in how models vs. humans are distracted by irrelevant features ([LLMs as Models for Analogical Reasoning](https://arxiv.org/html/2406.13803v2#:~:text=analogical%20reasoning%20tasks%20that%20require,though%20humans%20and%20LLMs%20respond)) ([LLMs as Models for Analogical Reasoning](https://arxiv.org/html/2406.13803v2#:~:text=reasoning%20from%20semantic%20structure%20and,actually)). CLUTRR was introduced around 2019 and small models struggled with it; newer LLMs do much better, though not perfect, especially as the number of relational steps increases. These tasks aren’t yet part of standard benchmarks like MMLU or BIG-Bench, but they appear in research probing the limits of LLM reasoning.  
- **Strengths:** They directly test an LLM’s ability to handle **abstract relationships**, not just surface content. A model that performs well on a difficult analogy (especially one it likely never saw verbatim) is probably engaging its capacity for **structural reasoning**, which is a strong indicator of general intelligence. These tasks are also domain-agnostic in principle – you can create analogies out of any content – so they allow evaluation of reasoning independent of specific knowledge domains. They also often require **few-shot reasoning** (learning from a couple examples or using the analogy structure itself as guidance), highlighting a model’s **cognitive flexibility** in applying known patterns to new situations.  
- **Limitations:** Designing good analogical tasks is hard – one must ensure the model hasn’t seen the example analogies in training (to truly test reasoning) and that the task is clearly defined enough that a correct answer is unambiguous. Sometimes models can bypass true analogy by using superficial cues (for example, if the analogy shares a common word or if one side is much more obvious, the model might guess without deeply mapping). Moreover, evaluating open-ended analogies can require judging if an answer is *valid* (there may be multiple valid analogies – e.g., some analogies can be completed in more than one way). For relational logic tasks, one limitation is that current large models actually do quite well on simpler ones, so to challenge them you need longer chains or noisier stories – which then start to overlap with general reading comprehension and memory challenges. This makes it tricky to pinpoint whether a wrong answer is due to failed reasoning or just confusion or a long context. Despite these issues, **analogical reasoning** remains a **promising evaluation angle**, since it inherently demands going beyond rote learning. Ongoing research (including efforts to build large analogy knowledge bases ([](https://aclanthology.org/2024.acl-long.68.pdf#:~:text=Abstract%20Analogical%20reasoning%20is%20a,of%20analogies%20from%20the%20KGs)) ([](https://aclanthology.org/2024.acl-long.68.pdf#:~:text=two%20analogical%20reasoning%20tasks%20,Resources%20of%20this%20paper%20can))) will likely yield more robust analogy benchmarks for LLMs.

### Common Sense & Practical Reasoning Benchmarks  
Another category focuses on everyday reasoning and understanding of the world, which LLMs often pick up from text corpora:

- **Winograd Schema Challenge (WSC) and Variants:** A Winograd Schema is a pair of sentences that differ by one word and lead to a pronoun referring to different entities, testing contextual commonsense. Example: *“The city councilmen refused the demonstrators a permit because **they** feared violence.”* vs *“...because **they** advocated violence.”* In the first, “they” refers to councilmen; in the second, “they” refers to demonstrators. LLMs must use semantic and real-world knowledge to resolve the pronoun. **WinoGrande** is a large-scale version with many such problems.  
- **COPA (Choice of Plausible Alternatives):** A dataset of simple commonsense causal reasoning – given a premise and a question (cause or effect), choose between two alternatives the one that makes more sense. For example: *“The vase fell off the shelf. Q: Why? A: (A) There was an earthquake. (B) It was made of glass.”* (Correct: A, an earthquake *causes* the vase to fall; B is irrelevant.)  
- **Social/IQAx benchmarks:** Tasks asking models to reason about social situations, motivations, or what’s the appropriate behavior (e.g. **Social IQA** or **ETHICS** dataset). These test nuanced commonsense and ethical reasoning.

These benchmarks check if LLMs have a grasp of **cause and effect, motivation, physical commonsense, and social norms** purely from textual description.

- **Evaluates:** **Commonsense reasoning** – the ability to fill gaps that are obvious to humans but not explicitly stated. WSC targets **coreference disambiguation using world knowledge**; COPA targets understanding of everyday causality; social reasoning tasks target understanding of human intentions and norms.  
- **Scoring:** Objective (accuracy on the multiple-choice or binary choices).  
- **LLM Evaluation:** These were very challenging for pre-LLM systems. Large language models have dramatically improved the state of the art – for instance, GPT-3 and GPT-4 achieved very high accuracy on WinoGrande and COPA, nearing or surpassing human performance. They are commonly included in evaluation suites (e.g., SuperGLUE included COPA and a Winograd variant; many papers still report on them).  
- **Strengths:** They test **real-world reasoning** that isn’t neatly described by formal logic – exactly the kind of intuitive, associative reasoning that LLMs’ training on vast text might give them. Success on these indicates the model has not only linguistic ability but also a broad *implicit* understanding of how the world works and how people behave. They’re also straightforward to evaluate and don’t require any special prompting or formatting.  
- **Limitations:** Top-tier models are now so good at many commonsense tasks that these benchmarks have nearly saturated. For example, GPT-4 reportedly gets ~95%+ on Winograd-style tasks, which rivals the consistency of humans (who occasionally get these wrong too). At that point, these tasks no longer reveal differences between advanced models. Also, there’s the data contamination issue: many Winograd and COPA examples were likely seen during training (or at least the model saw similar sentence patterns), which makes it hard to tell if it’s reasoning or recalling the answer. In addition, while useful, these tasks cover only *short context* commonsense; they don’t necessarily test multi-step reasoning or higher-order abstraction. So, they’re necessary but not sufficient for evaluating “raw intelligence.”

### Cognitive Flexibility and Few-Shot Learning Tests  
One intriguing aspect of LLMs is their ability to perform **in-context learning** – adjusting to new instructions or examples on the fly. Some benchmarks explicitly measure this adaptability:

- **Linguistic rule induction:** We mentioned the BBEH Hyperbaton task, where the model sees sentences in a made-up dialect or an English variant and must infer the adjective order rule. Similarly, DeepMind’s **“Linguist’s Nightmare” (Linguini)** task gives a made-up mini-language with example translations and asks the model to translate a new sentence ([[2502.19187] BIG-Bench Extra Hard](https://ar5iv.org/pdf/2502.19187#:~:text=reasoning%2C%20geometric%20understanding%2C%20dealing%20with,language%20given%20some%20examples%20and)). These tasks check if an LLM can **learn a mini grammar or pattern from a few examples** (the way a human might in an IQ test puzzle).  
- **Multi-task adaptation:** Evaluations like **MMLU (Massive Multitask Language Understanding)** test a model on 57 different subjects (from history to chemistry to math) in zero-shot or few-shot mode. While primarily a knowledge test, a strong performance also indicates the model’s flexibility to jump between domains without fine-tuning.  
- **Interactive or dynamic tasks:** There are proposed evaluations where the rules can change mid-way through an interaction (e.g., first the model is told to follow one set of criteria, then later the instructions change to test if it can override prior assumptions).

- **Evaluates:** **Adaptability and generalization.** The model’s **cognitive flexibility** – can it handle novel tasks or rules that weren’t explicitly in training, using only minimal guidance? This is akin to how an IQ test might have a novel puzzle type that you have to figure out on the spot. For LLMs, this is a key aspect of “raw intelligence”: being able to apply learned reasoning strategies to new problems. Few-shot learning tasks also test **sample efficiency** – can the model grasp a concept from 2-3 demonstrations?  
- **Scoring:** Varies by task; could be accuracy of outputs given new rules, or success rate on tasks across different domains without specialized prompting.  
- **LLM Evaluation:** The ability to do this is often assessed qualitatively or via aggregate benchmarks like BIG-Bench (which inherently tests many tasks zero-shot). One formal approach is the idea of **Adaptive Testing** for LLMs ([Efficiently Measuring the Cognitive Ability of LLMs: An Adaptive Testing Perspective | OpenReview](https://openreview.net/forum?id=s6X3s3rBPW#:~:text=Abstract%3A%20Large%20language%20models%20,accurate%20estimation%20of%20the%20model%27s)) ([Efficiently Measuring the Cognitive Ability of LLMs: An Adaptive Testing Perspective | OpenReview](https://openreview.net/forum?id=s6X3s3rBPW#:~:text=human,in%20large%20language%20model%20evaluation)), where question difficulty is adjusted based on the model’s performance (much like an adaptive IQ test) to zero in on its capability. Such methods aren’t widespread yet, but research indicates they could efficiently pinpoint an LLM’s level (e.g. identifying that GPT-4 behaves like a “middle-level student” on math after a certain point of adaptive quizzing) ([Efficiently Measuring the Cognitive Ability of LLMs: An Adaptive Testing Perspective | OpenReview](https://openreview.net/forum?id=s6X3s3rBPW#:~:text=human,in%20large%20language%20model%20evaluation)).  
- **Strengths:** Demonstrates the model’s **general reasoning aptitude** in a very pure way – if an LLM can solve a puzzle or follow a novel rule that it’s never explicitly seen, that’s strong evidence of an underlying flexible problem-solving ability rather than just rote knowledge. Few-shot generalization is also exactly how we often want to use LLMs (giving a couple of examples and having it continue correctly), so benchmarks here measure practical capability. These tests also help distinguish models: some smaller or less advanced models might completely fail to adapt to a new format, whereas larger ones succeed, giving a clear comparison.  
- **Limitations:** Setting up a good adaptive or novel task test is complex. If the task is *too* novel or too difficult, even a generally intelligent model might be confused by the instructions (leading to a false negative on its ability). Conversely, if the task has any hints that resemble known tasks, the model might latch onto them and solve it without truly adapting (e.g., if our “made-up language” still resembles pig latin or something the model knows). Additionally, evaluating adaptability often requires interactive or multi-step prompting, which can introduce variance – the exact phrasing of examples or instructions can strongly affect outcomes. Despite these challenges, pushing on this front is very important: true general AI would excel at adapting to new problems, so we need ways to measure progress toward that.

---

## Discussion: Towards Measuring “Raw” LLM Intelligence

Benchmarks that emphasize **associative agility, verbal reasoning, analogical mapping, and creative problem solving** appear far more aligned with LLMs’ innate strengths than sole reliance on math word problems. Math benchmarks like GSM8K are valuable for testing specific logical discipline (and for revealing shortcomings, as GSM-Symbolic showed ([[2410.05229] GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models](https://arxiv.org/abs/2410.05229#:~:text=the%20limitations%20of%20existing%20evaluations%2C,of%20mathematical%20reasoning%20in%20these)) ([Benchmarking Reasoning Robustness in Large Language Models](https://arxiv.org/html/2503.04550#:~:text=while%20they%20can%20simulate%20abstract,performance%2C%20highlighting%20the%20robustness%20deficiency))), but they tap a narrow skill set often improved by brute-force pattern learning or external tool use. In contrast, an LLM’s training on vast text endows it with rich **world knowledge, linguistic nuance, and the ability to draw connections** – exactly what analogies, verbal puzzles, and diverse reasoning tasks leverage.

**Analogical reasoning** in particular stands out as a promising evaluation avenue. It requires a model to **abstract a relationship** and apply it elsewhere, something we see hints of LLMs doing (for example, GPT-4 generating apt analogies in explanations). A well-designed analogical benchmark (drawing perhaps on human tests like MAT/JCWS and new synthetic tasks) can objectively score how often the model finds the correct mapping. Recent research confirms LLMs can handle certain analogies but still falter on more complex, abstract ones ([LLMs as Models for Analogical Reasoning](https://arxiv.org/html/2406.13803v2#:~:text=between%20different%20domains%E2%80%94is%20fundamental%20to,This)) ([LLMs as Models for Analogical Reasoning](https://arxiv.org/html/2406.13803v2#:~:text=reasoning%20from%20semantic%20structure%20and,actually%20explanations)), meaning this domain can distinguish levels of “thinking” ability. We should expect future benchmarks to include analogy challenges not just with words but with larger structures (paragraph analogies, scenario analogies) ([[PDF] A Novel Benchmark for Long Text Analogy Evaluation in Large ...](https://aclanthology.org/2023.findings-acl.218.pdf#:~:text=,sentences%2C)), truly testing an LLM’s ability to **transfer knowledge between contexts**.

**Verbal creativity and associativity** are another underexplored dimension. The fact that GPT-4 can score in the 99th percentile of human originality ([
The originality of machines: AI takes the Torrance Test – DOAJ](https://doaj.org/article/8f092ccc969840d181456110ec35bc2c#:~:text=This%20exploratory%20research%20investigated%20the,additional%20research%20to%20further%20define)) is astonishing – it suggests current models have a well of generative power that we lack metrics for, beyond ad-hoc “write a poem” demos. Developing systematic tests for creativity (e.g. a standardized set of tasks to generate analogies or brainstorm ideas, scored by diversity and novelty) could quantify this strength. Such tests would measure **cognitive flexibility** and **divergent thinking**, rewarding models that produce a wide range of sensible, novel outputs. This is tricky to score automatically, but one could imagine using panels of human judges or proxy metrics (as done in the Torrance tests research) to evaluate LLM outputs. As AI systems are increasingly used for creative work, having a “creativity benchmark” would be highly relevant.

Finally, it’s important to note that **no single benchmark will capture “raw intelligence.”** Just as human IQ tests use a battery of subtests, we should evaluate LLMs along multiple axes – analogical reasoning, logical deduction, commonsense, learning new tasks, creativity, and yes, some math – to form a holistic picture. Benchmarks like BIG-Bench were a first attempt at this multi-faceted evaluation, and BBEH is a strong next step. We will likely see more **comprehensive evaluation suites** that incorporate human-designed test items, synthetic challenges, and even interactive tasks (for example, having a model engage in a dialogue that tests its reasoning under various conditions). 

In conclusion, moving beyond grade-school math problems toward **language-rich, knowledge-informed reasoning tests** can better measure an LLM’s *general-purpose reasoning prowess*. Human verbal IQ tests (analogies, similarities, etc.) offer time-proven formats for assessing abstract reasoning and could be adapted for AI, while AI-tailored benchmarks (BIG-Bench, BBEH, and other emerging challenges) push models into the realm of complex reasoning and adaptation. Together, these evaluations target the essence of “mental horsepower” in LLMs – the ability to **understand, reason, and generalize** in the flexible, creative way that humans consider intelligent. By regularly testing models with such benchmarks, we can more reliably track real progress in AI’s general reasoning capabilities, ensuring that improvements aren’t just artifacts of solving the quirks of a single dataset, but reflect genuinely smarter and more versatile language models.

**Sources:** Recent literature and benchmarks were referenced to substantiate this exploration, including the GSM-Symbolic study on math reasoning limitations ([[2410.05229] GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models](https://arxiv.org/abs/2410.05229#:~:text=the%20limitations%20of%20existing%20evaluations%2C,of%20mathematical%20reasoning%20in%20these)), discussions of LLMs vs. human IQ tests ([Benchmarking Reasoning Robustness in Large Language Models](https://arxiv.org/html/2503.04550#:~:text=Jiang%20et%20al,performance%2C%20highlighting%20the%20robustness%20deficiency)) ([Benchmarking Reasoning Robustness in Large Language Models](https://arxiv.org/html/2503.04550#:~:text=while%20they%20can%20simulate%20abstract,performance%2C%20highlighting%20the%20robustness%20deficiency)), descriptions of cognitive tests ([Miller Analogies Test - Wikipedia](https://en.wikipedia.org/wiki/Miller_Analogies_Test#:~:text=The%20test%20aimed%20to%20measure,test%20question%20might%20have%20been)) ([Verbal Comprehension in WAIS: Skills and Subtests | Cogn-IQ.org](https://www.cogn-iq.org/wais-verbal-comprehension.php#:~:text=2)) ([Assess Verbal Reasoning with the JCWS | Cogn-IQ](https://www.cogn-iq.org/word-similarities-iq-test.php#:~:text=The%20JCWS%20test%20assesses%20several,key%20verbal%20reasoning%20skills)), and the introduction of new benchmarks like BBEH ([[2502.19187] BIG-Bench Extra Hard](https://arxiv.org/abs/2502.19187#:~:text=skills%20within%20a%20unified%20framework,for%20the%20best)). The performance gaps and needs identified underscore the importance of diverse, robust evaluation methods for LLMs moving forward.

Full Link: https://chatgpt.com/share/67edcd28-36e0-8002-8f9d-af26a66a600e
